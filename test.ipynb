{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['BasicTokenizer', 'EarlyStopping', 'In', 'Out', 'Path', 'STOP_WORDS_PATH', 'VOCAB_PATH', 'VocabGenerator', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'os', 'pad_sequence_to_fixed_length', 'quit']\n"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from nlputils import *\n",
    "\n",
    "VOCAB_PATH = Path('test_vocab.txt')\n",
    "STOP_WORDS_PATH = Path('test_stopwords.txt')\n",
    "\n",
    "print(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Hello', ',', 'how', 'are', 'you', '?', 'What', \"'s\", 'your', 'name', '?', 'I', 'am', 'Steven Jobs', '.', 'Emily Jackson', 'is', 'my', 'wife', '.']\n['hello', ',', 'how', 'be', '-PRON-', '?', 'what', 'be', '-PRON-', 'name', '?', '-PRON-', 'be', 'Steven Jobs', '.']\n['one', 'ever', 'afterwards', \"'ve\", '’s', 'mine', 'several', 'moreover', 'fifteen', 'thereafter']\n"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(language='en', lemma=False)\n",
    "print(tokenizer.tokenize(\"Hello, how are you? What's your name? I am Steven Jobs. Emily Jackson is my wife.\"))\n",
    "tokenizer = BasicTokenizer(language='en', lemma=True)\n",
    "print(tokenizer.tokenize(\"Hello, how are you? What's your name? I am Steven Jobs.\"))\n",
    "print(tokenizer.get_stopwords()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['不然', '因了', '──', '另悉', '咋', '否则', '倘', '⑨', '如此', '嘿嘿']\n"
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(language='cn')\n",
    "print(tokenizer.get_stopwords()[:10])\n",
    "\n",
    "samples = ['《荒野大镖客：救赎2》拥有一个巨大的开放世界，而且充满活力，不过单人模式下在这个世界中逛久了，总是会感觉有些无聊。于是下面这位玩家Alex Tanaka决定让自己化身为西部大恶人', '他的做法就是绑架游戏中每个郡的治安官，然后在风景宜人的地方与他们玩决斗游戏。决斗的结果他会直接远景截图，当成风景照传到网上，当然他基本只发自己吊打对方的照片。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 30\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 30\n"
    }
   ],
   "source": [
    "sequence = list(range(20))\n",
    "padded_sequence = pad_sequence_to_fixed_length(sequence, max_length=30)\n",
    "print(padded_sequence, len(padded_sequence))\n",
    "padded_sequence = pad_sequence_to_fixed_length(sequence, max_length=30, padding_mode='left')\n",
    "print(padded_sequence, len(padded_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\LAGSAU~1\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 1.230 seconds.\nPrefix dict has been built successfully.\n['我', '只能', ' ', '搞笑', '了']\n['只能', ' ', '搞笑']\n"
    }
   ],
   "source": [
    "string = '我只能 搞笑了'\n",
    "print(tokenizer.tokenize(string, no_stop_words=False))\n",
    "print(tokenizer.tokenize(string, no_stop_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['hahah', 'qwer', 'w', '1234']\n"
    }
   ],
   "source": [
    "tokenizer.load_stopwords(str(STOP_WORDS_PATH))\n",
    "print(list(tokenizer.get_stopwords())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['《', '荒野', '大', '镖客', '：', '救赎', '2', '》', '拥有', '一个', '巨大', '的', '开放', '世界', '，', '而且', '充满活力', '，', '不过', '单人', '模式', '下', '在', '这个', '世界', '中逛', '久', '了', '，', '总是', '会', '感觉', '有些', '无聊', '。', '于是', '下面', '这位', '玩家', 'Alex', ' ', 'Tanaka', '决定', '让', '自己', '化身为', '西部', '大', '恶人'], ['他', '的', '做法', '就是', '绑架', '游戏', '中', '每个', '郡', '的', '治安', '官', '，', '然后', '在', '风景', '宜人', '的', '地方', '与', '他们', '玩', '决斗', '游戏', '。', '决斗', '的', '结果', '他会', '直接', '远景', '截图', '，', '当成', '风景', '照', '传到', '网上', '，', '当然', '他', '基本', '只发', '自己', '吊打', '对方', '的', '照片', '。']]\n"
    }
   ],
   "source": [
    "seg_samples = [tokenizer.tokenize(x) for x in samples]\n",
    "print(seg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['[BOS]', '[EOS]', '[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '的', '，', '。', '大', '世界', '在', '自己', '他', '游戏', '风景', '决斗', '《', '荒野']\nCounter({'的': 6, '，': 6, '。': 3, '大': 2, '世界': 2, '在': 2, '自己': 2, '他': 2, '游戏': 2, '风景': 2, '决斗': 2, '《': 1, '荒野': 1, '镖客': 1, '：': 1, '救赎': 1, '2': 1, '》': 1, '拥有': 1, '一个': 1, '巨大': 1, '开放': 1, '而且': 1, '充满活力': 1, '不过': 1, '单人': 1, '模式': 1, '下': 1, '这个': 1, '中逛': 1, '久': 1, '了': 1, '总是': 1, '会': 1, '感觉': 1, '有些': 1, '无聊': 1, '于是': 1, '下面': 1, '这位': 1, '玩家': 1, 'Alex': 1, ' ': 1, 'Tanaka': 1, '决定': 1, '让': 1, '化身为': 1, '西部': 1, '恶人': 1, '做法': 1, '就是': 1, '绑架': 1, '中': 1, '每个': 1, '郡': 1, '治安': 1, '官': 1, '然后': 1, '宜人': 1, '地方': 1, '与': 1, '他们': 1, '玩': 1, '结果': 1, '他会': 1, '直接': 1, '远景': 1, '截图': 1, '当成': 1, '照': 1, '传到': 1, '网上': 1, '当然': 1, '基本': 1, '只发': 1, '吊打': 1, '对方': 1, '照片': 1})\n"
    }
   ],
   "source": [
    "gen = VocabGenerator()\n",
    "gen.generate_vocab(seg_samples)\n",
    "vocab = gen.get_vocab()\n",
    "gen.save_vocab_to(str(VOCAB_PATH))\n",
    "\n",
    "print(vocab[:20])\n",
    "print(gen.get_token2tf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['[BOS]', '[EOS]', '[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '的', '，', '。', '大', '世界', '在', '自己', '他', '游戏', '风景', '决斗']\n"
    }
   ],
   "source": [
    "gen = VocabGenerator(min_count=1)   # Discards words with low frequencies.\n",
    "gen.generate_vocab(seg_samples)\n",
    "print(gen.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['[BOS]', '[EOS]', '[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '的', '，', '。', '大', '世界', '在', '自己', '他', '游戏', '风景', '决斗', '《', '荒野', '镖客', '：', '救赎', '2', '》', '拥有', '一个', '巨大', '开放', '而且', '充满活力', '不过', '单人', '模式', '下', '这个', '中逛', '久', '了', '总是', '会', '感觉', '有些', '无聊', '于是', '下面', '这位', '玩家', 'Alex', ' ', 'Tanaka', '决定', '让', '化身为', '西部', '恶人', '做法', '就是', '绑架', '中', '每个', '郡', '治安', '官', '然后', '宜人', '地方', '与', '他们', '玩', '结果', '他会', '直接', '远景', '截图', '当成', '照', '传到', '网上', '当然', '基本', '只发', '吊打', '对方', '照片']\n"
    }
   ],
   "source": [
    "tokenizer.load_vocab(vocab)\n",
    "print(list(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('[BOS]', 0), ('[EOS]', 1), ('[UNK]', 2), ('[SEP]', 3), ('[PAD]', 4), ('[CLS]', 5), ('[MASK]', 6), ('的', 7), ('，', 8), ('。', 9), ('大', 10), ('世界', 11), ('在', 12), ('自己', 13), ('他', 14), ('游戏', 15), ('风景', 16), ('决斗', 17), ('《', 18), ('荒野', 19)]\n"
    }
   ],
   "source": [
    "token2id = tokenizer.get_token2id()\n",
    "print(list(token2id.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[(0, '[BOS]'), (1, '[EOS]'), (2, '[UNK]'), (3, '[SEP]'), (4, '[PAD]'), (5, '[CLS]'), (6, '[MASK]'), (7, '的'), (8, '，'), (9, '。'), (10, '大'), (11, '世界'), (12, '在'), (13, '自己'), (14, '他'), (15, '游戏'), (16, '风景'), (17, '决斗'), (18, '《'), (19, '荒野')]\n"
    }
   ],
   "source": [
    "print(list(tokenizer.get_id2token().items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1 = tokenizer.convert_tokens_to_ids(seg_samples[0])\n",
    "ids2 = tokenizer.encode(samples[0])\n",
    "print(ids1)\n",
    "print(ids2)\n",
    "ids1 == ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29]\n[43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n"
    }
   ],
   "source": [
    "print(tokenizer.encode(samples[0], max_length=16, truncate_mode='right'))\n",
    "print(tokenizer.encode(samples[0], max_length=16, truncate_mode='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n"
    }
   ],
   "source": [
    "print(tokenizer.encode(samples[0], max_length=70, padding_mode='right'))\n",
    "print(tokenizer.encode(samples[0], max_length=70, padding_mode='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "《荒野大镖客：救赎2》拥有一个巨大的开放世界，而且充满活力，不过单人模式下在这个世界中逛久了，总是会感觉有些无聊。于是下面这位玩家Alex Tanaka决定让自己化身为西部大恶人\n《荒野大镖客：救赎2》拥有一个巨大的开放世界，而且充满活力，不过单人模式下在这个世界中逛久了，总是会感觉有些无聊。于是下面这位玩家Alex Tanaka决定让自己化身为西部大恶人\n"
    }
   ],
   "source": [
    "print(tokenizer.decode(ids1))\n",
    "print(tokenizer.decode(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2, 2, 2, 9]\n[UNK][UNK][UNK]。\n"
    }
   ],
   "source": [
    "sample = '你好啊小老弟。'\n",
    "ids = tokenizer.encode(sample)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitwgaconda403d83615d4c4c99b38d0701e344bb93",
   "display_name": "Python 3.7.4 64-bit ('wga': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}