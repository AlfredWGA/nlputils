{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_PATH = Path('.')\n",
    "\n",
    "VOCAB_PATH = ROOT_PATH.joinpath('data/vocab.txt')\n",
    "\n",
    "STOP_WORDS_PATH = ROOT_PATH.joinpath('data/哈工大停用词表.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils.vocab_generator import VocabGenerator\n",
    "from nlputils.tokenizer import BasicTokenizer, pad_sequence_to_fixed_length\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(name)s %(levelname)s %(message)s\",\n",
    "                    datefmt='%Y-%m-%d  %H:%M:%S %a')\n",
    "\n",
    "tokenizer = BasicTokenizer(language='cn')\n",
    "\n",
    "gen = VocabGenerator(coverage=1.0)\n",
    "\n",
    "samples = ['《荒野大镖客：救赎2》拥有一个巨大的开放世界，而且充满活力，不过单人模式下在这个世界中逛久了，总是会感觉有些无聊。于是下面这位玩家Alex Tanaka决定让自己化身为西部大恶人', '他的做法就是绑架游戏中每个郡的治安官，然后在风景宜人的地方与他们玩决斗游戏。决斗的结果他会直接远景截图，当成风景照传到网上，当然他基本只发自己吊打对方的照片。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]30\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]30\n"
    }
   ],
   "source": [
    "sequence = list(range(20))\n",
    "padded_sequence = pad_sequence_to_fixed_length(sequence, max_length=30)\n",
    "print(padded_sequence, len(padded_sequence))\n",
    "padded_sequence = pad_sequence_to_fixed_length(sequence, max_length=30, padding_mode='left')\n",
    "print(padded_sequence, len(padded_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2020-02-07  11:55:30 Fri nlputils.tokenizer INFO Load stop words from data\\哈工大停用词表.txt.\n"
    },
    {
     "data": {
      "text/plain": "['', '啦', '［②ｉ］', '吓', '照', '是的', '极了', '②ｃ', '\\t', '为什么']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.load_stopwords(str(STOP_WORDS_PATH))\n",
    "list(tokenizer.get_stopwords())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Building prefix dict from the default dictionary ...\n2020-02-07  11:55:30 Fri jieba DEBUG Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\LAGSAU~1\\AppData\\Local\\Temp\\jieba.cache\n2020-02-07  11:55:30 Fri jieba DEBUG Loading model from cache C:\\Users\\LAGSAU~1\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 1.241 seconds.\n2020-02-07  11:55:31 Fri jieba DEBUG Loading model cost 1.241 seconds.\nPrefix dict has been built succesfully.\n2020-02-07  11:55:31 Fri jieba DEBUG Prefix dict has been built succesfully.\n['我', '只能', ' ', '搞笑', '了']\n['只能', '搞笑']\n"
    }
   ],
   "source": [
    "string = '我只能 搞笑了'\n",
    "print(tokenizer.tokenize(string, no_stop_words=False))\n",
    "print(tokenizer.tokenize(string, no_stop_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['《', '荒野', '大', '镖客', '：', '救赎', '2', '》', '拥有', '一个', '巨大', '的', '开放', '世界', '，', '而且', '充满活力', '，', '不过', '单人', '模式', '下', '在', '这个', '世界', '中逛', '久', '了', '，', '总是', '会', '感觉', '有些', '无聊', '。', '于是', '下面', '这位', '玩家', 'Alex', ' ', 'Tanaka', '决定', '让', '自己', '化身为', '西部', '大', '恶人'], ['他', '的', '做法', '就是', '绑架', '游戏', '中', '每个', '郡', '的', '治安', '官', '，', '然后', '在', '风景', '宜人', '的', '地方', '与', '他们', '玩', '决斗', '游戏', '。', '决斗', '的', '结果', '他会', '直接', '远景', '截图', '，', '当成', '风景', '照', '传到', '网上', '，', '当然', '他', '基本', '只发', '自己', '吊打', '对方', '的', '照片', '。']]\n"
    }
   ],
   "source": [
    "seg_samples = [tokenizer.tokenize(x) for x in samples]\n",
    "print(seg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['[PAD]',\n '[BOS]',\n '[EOS]',\n '[UNK]',\n '[CLS]',\n '[SEP]',\n '[MASK]',\n '的',\n '，',\n '。',\n '大',\n '世界',\n '在',\n '自己',\n '他',\n '游戏',\n '风景',\n '决斗',\n '《',\n '荒野']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate_vocab(seg_samples)\n",
    "vocab = gen.get_vocab()\n",
    "gen.save_vocab_to(str(VOCAB_PATH))\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['照',\n '无聊',\n '玩家',\n '他会',\n '的',\n '绑架',\n '照片',\n '游戏',\n '有些',\n '每个',\n '宜人',\n '地方',\n '对方',\n '充满活力',\n '[MASK]',\n '决定',\n '基本',\n '。',\n '[SEP]',\n '他']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.load_vocab(vocab)\n",
    "list(tokenizer.get_vocab())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('[PAD]', 0),\n ('[BOS]', 1),\n ('[EOS]', 2),\n ('[UNK]', 3),\n ('[CLS]', 4),\n ('[SEP]', 5),\n ('[MASK]', 6),\n ('的', 7),\n ('，', 8),\n ('。', 9),\n ('大', 10),\n ('世界', 11),\n ('在', 12),\n ('自己', 13),\n ('他', 14),\n ('游戏', 15),\n ('风景', 16),\n ('决斗', 17),\n ('《', 18),\n ('荒野', 19),\n ('镖客', 20),\n ('：', 21),\n ('救赎', 22),\n ('2', 23),\n ('》', 24),\n ('拥有', 25),\n ('一个', 26),\n ('巨大', 27),\n ('开放', 28),\n ('而且', 29),\n ('充满活力', 30),\n ('不过', 31),\n ('单人', 32),\n ('模式', 33),\n ('下', 34),\n ('这个', 35),\n ('中逛', 36),\n ('久', 37),\n ('了', 38),\n ('总是', 39),\n ('会', 40),\n ('感觉', 41),\n ('有些', 42),\n ('无聊', 43),\n ('于是', 44),\n ('下面', 45),\n ('这位', 46),\n ('玩家', 47),\n ('Alex', 48),\n (' ', 49),\n ('Tanaka', 50),\n ('决定', 51),\n ('让', 52),\n ('化身为', 53),\n ('西部', 54),\n ('恶人', 55),\n ('做法', 56),\n ('就是', 57),\n ('绑架', 58),\n ('中', 59),\n ('每个', 60),\n ('郡', 61),\n ('治安', 62),\n ('官', 63),\n ('然后', 64),\n ('宜人', 65),\n ('地方', 66),\n ('与', 67),\n ('他们', 68),\n ('玩', 69),\n ('结果', 70),\n ('他会', 71),\n ('直接', 72),\n ('远景', 73),\n ('截图', 74),\n ('当成', 75),\n ('照', 76),\n ('传到', 77),\n ('网上', 78),\n ('当然', 79),\n ('基本', 80),\n ('只发', 81),\n ('吊打', 82),\n ('对方', 83),\n ('照片', 84)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id = tokenizer.get_token2id()\n",
    "list(token2id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, '[PAD]'),\n (1, '[BOS]'),\n (2, '[EOS]'),\n (3, '[UNK]'),\n (4, '[CLS]'),\n (5, '[SEP]'),\n (6, '[MASK]'),\n (7, '的'),\n (8, '，'),\n (9, '。')]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.get_id2token().items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1 = tokenizer.convert_tokens_to_ids(seg_samples[0])\n",
    "ids2 = tokenizer.encode(samples[0])\n",
    "print(ids1)\n",
    "print(ids2)\n",
    "ids1 == ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29]\n[43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n"
    }
   ],
   "source": [
    "print(tokenizer.encode(samples[0], max_length=16, truncate_mode='right'))\n",
    "print(tokenizer.encode(samples[0], max_length=16, truncate_mode='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 7, 28, 11, 8, 29, 30, 8, 31, 32, 33, 34, 12, 35, 11, 36, 37, 38, 8, 39, 40, 41, 42, 43, 9, 44, 45, 46, 47, 48, 49, 50, 51, 52, 13, 53, 54, 10, 55]\n"
    }
   ],
   "source": [
    "print(tokenizer.encode(samples[0], max_length=70, padding_mode='right'))\n",
    "print(tokenizer.encode(samples[0], max_length=70, padding_mode='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "《荒野大镖客：救赎2》拥有一个巨大的开放世界，而且充满活力，不过单人模式下在这个世界中逛久了，总是会感觉有些无聊。于是下面这位玩家Alex Tanaka决定让自己化身为西部大恶人\n《荒野大镖客：救赎2》拥有一个巨大的开放世界，而且充满活力，不过单人模式下在这个世界中逛久了，总是会感觉有些无聊。于是下面这位玩家Alex Tanaka决定让自己化身为西部大恶人\n"
    }
   ],
   "source": [
    "print(tokenizer.decode(ids1))\n",
    "print(tokenizer.decode(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[3, 3, 3, 9]\n[UNK][UNK][UNK]。\n"
    }
   ],
   "source": [
    "sample = '你好啊小老弟。'\n",
    "ids = tokenizer.encode(sample)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-candidate"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bitd9c374a636ea4addbd47e9e0a59eb730",
   "display_name": "Python 3.6.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}